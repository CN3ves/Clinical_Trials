# Types of Trial Designs

## Phases of trials

Before we discuss trial design, I thought I'd introduce some
terminology that you're likely to hear about phases of clinical trials.
So the phases of clinical trials refers to the sequence
of trials that are necessary to bring a treatment into use.
The terms originally came from drug development,
but had been extrapolated to other types of trials.

* Phase one trials are the first stage in **testing** a new intervention in humans. They're usually **small** with only 10 to 30 people, and they might include healthy volunteers or those with disease. The goal of phase one trials is to **identify** a tolerable dose, and also to provide information on drug metabolism and excretion and really gather information
on toxicities. Phase one studies are frequently **not controlled**. So we are not going to discuss designs for phase one studies in this class.
* Phase two studies are **slightly larger**, they usually have 30 to 100 people. And in phase two we start to **collect preliminary information** on efficacy, but we continue to collect information on side effects and safety. Phase two trials are sometimes controlled and sometimes uncontrolled. 
* Phase three trials are the final approval stage for drug trials. They usually involve a 100 or more people, and the goal is to **assess** both efficacy and safety. Phase three trials are **controlled** and almost always **randomized**.

Phase one and two are usually considered the learning phases
of clinical trials, and phase three is considered the demonstration phase.
You sometimes hear of phase four studies. And these are studies of a drug or device
after market approval. The objective of phase four studies is to
see how an intervention works in the real world, and to examine long term safety.
Phase four studies are frequently observational, but sometimes they are control trials.


## Comparison Structure


We are going to talk
about the comparison structure of a trial and the
comparison structure describes the different ways that we compare
an experimental group to a control group in the trial.
The general types of comparison structures are
parallel, crossover and group allocation.

### Parallel Design 
We'll start first with the parallel design.
This is the design that we usually think of when we think of a clinical trial.
* In a parallel design, we are assigning patients and administering treatment, so the experimental and control groups in parallel. In other words, we are assigning people to both
groups, over the same period of time, as opposed to collecting data on the experimental groups
only, and comparing that data to historical controls, or as opposed to assigning treatment A and then assigning treatment B in series.
* And each person in a parallel design is designed to only one treatment group. 
* The process by which we allocate people to a specific treatment group is usually by randomization. We use randomization to allocate patients because it removes bias in the allocation process which is called selection bias. Randomization also has the advantage of producing comparable groups with respect to known and unknown factors on average. 
* Our statistical comparison for the parallel design, is the comparison of a summary of the outcome measures between the treatment groups.

A depicting a parallel design we start with the defined population
or the group of people who are eligible for our trial.
Then through an allocation process, usually randomization, we
decide who is assigned to which treatment group.
As shown here we are deciding who will be assigned to receive
the new treatment versus who will be assigned to receive the current treatment.
We see that we will observe
after some period of time, who among those
assigned to new treatment has improved and who has
not improved, and who among those assigned to
current treatment has improved and who has not improved.
We use this observation to make our comparison
of efficacy and safety among the treatment groups.

#### Example: NETT trial
An example of a clinical trial with a parallel
design is the National Emphysema Treatment Trial or NETT.
NETT was a phase three trial conducted in the late 1990s and early 2000s.
NETT enrolled about 12 hundred people with severe emphysema.
If you read the NETT design papers that are referenced here at
the bottom of the slide, you'll notice that NETT was originally designed
to be more than twice as large, but the sample size was
recalculated based on higher than expected mortality rate, and a lower than expected
rate of drop ins and drop outs.
The participants were randomly allocated to one of two treatment groups.
The experimental treatment was a lung volume
reduction surgery in addition to standard medical therapy.
In lung volume reduction surgery, the most severely damaged lung tissue is removed so
that the remaining lung tissue and the
surrounding muscles are able to work more efficiently.
Lung volume reduction surgery was not a new technique when
that began, it was actually developed in the 1950s but originally the surgery
had a large mortality rate and it was not much pursued as a surgical intervention.
At the time of NETT lung volume reduction surgery
had been resurrected with new surgical techniques, and was
much safer, but the question remained as to whether
or not it improved lung function in people with emphysema.
The control treatment in NETT was
medical therapy only.
That is, people were managed by their primary
care physicians following the guidelines that were proposed by
the American Thoracic Society at the time, which included
smoking cessation, oxygen therapy, and other therapies and medications.

NETT was an unmasked trial, so there was no sham surgery in the control group.

NETT was designed to test superiority, either the surgery group was superior
to the control group, or the control group was superior to the surgery group.
The primary outcome was mortality and exercise capacity.
And there were numerous secondary outcomes such as quality of life,
the rate of symptoms, lung function and mechanics and functional capacity.
There was a long period of follow up.
Some of the participants had as much as seven and a half years of follow up.
The participants were recruited and followed
at 17 centers in the United States, and the trial also
had resource centers, such as a
coordinating center and a coordinating center
was here at Johns Hopkins Bloomberg School of Public Health, and
a project office at NHLBI, the national heart, lung and blood institute.

### Crossover Design
Now we will leave the parallel design and
move onto another design called a crossover design.
* In a crossover design the unit that is randomized is the order in which the treatments are received, instead of whether or not the patient receives A or B. So in a crossover design we randomize whether they receive A first and then B or B and then A. And so in this case, randomization promotes balance between the treatment groups and timing of the exposure. 
* The defining feature of our crossover design is that we're testing each
treatment in all patients. That means that each patient serves as his or her own control.
This is a nice feature because variability is almost always higher between measurements of
an outcome taken on different people, than in repeated measurements taken on the same person.
If each person serves his or her own control, we are essentially controlling for other person level characteristics that may affect outcome measurement and increase variability.
* As a result of the reduction in variability, we need fewer patients to test the hypothesis of interest. So the crossover design is more efficient than the comparable parallel design.

On this slide, we have a graphical representation of the cross over design.
Here group one is represented by a dashed line
and group two is represented by a solid line.
One the top left, we see the group one receives treatment A first
followed by a washout period that is represented here with a line green color.
And then group 1 receives treatment B.
Group two receives treatment B first, followed by a
washout period, and then group two receives treatment A.


The crossover design is appealing because of its efficiency, however the design
has many disadvantages that really limit its utility.
Most importantly, only a specific kind of medical condition
and treatment are appropriate for study with a crossover design.
* None of the treatments in the study can provide a permanent cure. If a treatment has a permanent cure, then we can't cross somebody over to the other treatment, because they've already reached the outcome. So the conditions for which we can use a crossover design are only those that have a chronic level of intensity for which the treatments provide symptomatic relief but not any permanent cure. 
* Another concern with this design is the potential for a treatment in any early period to have effects that carry over to later periods. For instance, if I'm a person randomized to
receive treatment A, and then B, I might have some effects from treatment A, that get carried over to the time period when I'm receiving treatment B. The result of this is that we need some
period of time between the administrations of the treatments, to allow the effects of the treatments in the earlier periods, to subside before beginning the administration of the later treatments. That period of time is referred to as the **washout period**. The washout period needs to be **long enough** to ensure that we have no carryover effects of any of the treatments. So, this implies that the washout period has to be long enough to accommodate the treatment that has the longest potential carryover effect. Another issue that needs to be considered with a crossover is, issue of how we're going to **treat people** while they're in the washout period. 
* We can actually test to see if there are any period by treatment interactions. That is, does the order of administration affect the efficacy of treatment? Is A more effective when receive second, as opposed to first? So, we are testing for the carryover effect. Test for interaction are not nearly as powerful as tests for main effects. So the test to make sure that there is no carryover effect, is not a very powerful test.
* Another disadvantage of the crossover design, is that dropouts can be more problematic in
this design, because if you lose a participant you lose information from all treatments
for that participant.
* The crossover design also has slightly more complicated analysis requirements than a parallel design, because you have to account for the fact that you have correlated outcomes on the individuals.


So all of these limitations mean that crossover designs are feasible for the study
of chronic conditions that have a **constant level of intensity** of the underlying disease.
A condition for which you'd expect the intensity to resume, to it's regular level during the
washout period between the treatment administrations. So examples of chronic diseases for which this design is used are asthma, hypertension and sometimes arthritis or other pain relief studies. The treatments in a crossover design as I mentioned must have **short term effects** and relieve only the signs and symptoms of the disease, but they really shouldn't
have any permanent effect on the underlying disease process. Crossover designs are also used sometimes when we are in the **early phases of studying** a drug and we are looking at the
metabolic bioavailability or tolerability of a new product.


#### Examples 
I briefly searched PubMed for some examples of recent clinical trial publications that used crossover design. In this first example, the investigators were comparing an evening dose versus a morning dose of travoprost, in an open-angle glaucoma for 24 hour intraocular pressure control. So the patients were randomized to receive either in the
evening dose followed by the morning dose travoprost or vice versa. And they compared their 24 hour intraocular pressure control in the two groups. And in this study we are looking at the outcome of glaucoma, which is a chronic condition.

In the second example, the investigators were comparing montelukast versus salmeterol
as an adjuvant to inhaled fluticasone for exercise-induced asthma in children. So the investigators were studying add on therapies for children with asthma, which is a chronic condition, and the children were already taking fluticasone so they had a treatment to get them through those wash out periods.

In the last example, we have Neuragen PN, which is a topical oil, versus placebo for neuropathic pain. And in this example, the investigators were studying chronic neuropathic pain from peripheral neuropathy, and the participants were randomized to either the neuragen oil, or a placebo oil, and then vice versa, the participants were randomized to receive either the neuragen oil first and then the placebo oil or vice versa, as an agement to their current medication regimen.

### Group allocation Design 
Before we move on to the next design, I'd like to take the moment to discuss randomization units. The randomization unit is the level of which the randomization is applied. Usually, we're thinking the randomization unit as a person, that's what we have discussed so far, we
randomize a person to receive A or B, or in the crossover design, A first and then B.
But in reality it's not always the person that is the randomization unit even in a parallel design. For instance, you could randomize eyes within a person to receive A or B. 

* In a group allocation design, the randomization unit is a whole group of individuals, such as a community, or a school, or a clinic. The entire group of individuals is allocated to the same intervention. This type of randomization is also sometimes called cluster randomization.
* We use cluster randomization when individual randomization is not
practically feasible (**tracking**), or when it's thought to be unacceptable.
Another complication of using individual randomization
in a group setting, is **contamination**. For example if you're providing a behavioral intervention to several groups of school children, it will be difficult to administer the intervention to one single child in a classroom, without effecting the other children in the same classroom. In that case it might be more reasonable to randomize the entire class or the entire school to the specific intervention. 
* It's also important to recognize that if there's correlation in the responses within a
group, and there usually is, this design, the group allocation design, loses some efficiency. In other words, the group allocation design requires more individuals to address a given hypothesis, as compared to a parallel or a crossover design.

In this graphic I'm going to depict the group allocation design.In the top of the graph we have groups, and in the group allocation design the groups are frequently paired
based on some characteristics of the groups. So at the top we're going to start with paired groups. So we may have a pair of similar villages, or a pair of similar classrooms, and within
the pair, one class is randomized to the intervention, and one class is randomized to no intervention. Then across the boxes at the bottom, we follow these groups to see whether
or not they improve or not improve, and we compare the rates of improvement
across the groups.

#### Example: Sommer vitamin A trial.

This is a well known example at. Okay, this is a well known example with the students and
faculty at John's Hopkins, this is the Sommer vitamin A trial. Alfred Sommer did a trial of vitamin A prophylaxis in an area of the world that had a high prevalence of malnutrition and vitamin A deficiency. The goal of this study was to see if Vitamin A supplements could reduce mortality in children. The population for this trial was preschool children in northern Sumatra in 1982 and 83. The treatments were Vitamin A supplementation during the study, or no
supplementation during the study but Vitamin A supplementation after the study. So the control group, received supplementation after the study had been completed. 
The clusters for this trial were villages, there were 450 villages, and they were selected using a survey sampling method called systematic sampling. 
And then after the villages were selected, each one was randomly allocated to receive supplementation during or after the study.

## Extensions of the Parallel Design

we are going to talk about some extensions of the parallel design.
Extensions that we're going to cover here are
the factorial design and the large simple design.

### Factorial design 

In the factorial design we are testing
two or sometimes more experimental interventions **simultaneously**.
So we test treatment A versus the control
for treatment A, and we test treatment B versus
the control for treatment B.
We test the treatments simultaneously, either because
it's **economical** to test the two treatments simultaneously
or because the design can be used to
test for **interaction** between treatments A and B.
Usually we use the factorial design for economical reasons.
It's actually very rare to design a factorial trial
for the express purpose of testing for an interaction.
In fact, we usually assume that there is no interaction
between the two treatments and that they have an independent mode of action.
This is often more plausible if we actually test for two separate outcomes.
An example of a factorial trial with
two different outcomes is the Physicians' Health Study.
The Physicians' Health Study was a primary prevention trial among physicians in which
beta carotene was compared to placebo for the outcome of cancer, and aspirin
was compared to placebo for the outcome of coronary artery disease.

The factorial design can be graphically represented by a two by two table.
The top row represents participants who were randomized
to receive treatment A, and the bottom row represents
participants who were randomized to not receive treatment
A or to receive the control for treatment A.
Similarly, the left column is the people randomized
to receive treatment B, and the right column is the
people that were randomized to receive the control for treatment B.
The cells in the two by two table represent the four different
combinations that are possible with two treatments each having its own control.
So on the top left, we have people who are receiving both A and B.
In the top right we have people who are receiving A and the control for B.
In the bottom
left we have people who are receiving B and the control for A, and in the
bottom right we have people who are receiving
the control for A and the control for B.

* when we do a factorial design we are usually interested in the estimation of the main effects assuming that the treatments do not have an interaction. To make these comparisons, we use the responses of the people and the margins of the table, and we'll go back to the graph in a moment to review these responses.
* In the case where we are interested in the interaction, we have to compare the responses in the cells instead of in the margins, and it's important to note that the test for interaction is usually not a powerful test. Unless the sample size is very large, we are likely to have difficulty reliably detecting an interaction between treatments A and B.

So let's go back to the two by two graph of the factorial design.
To assess the main effect of A, we compare the
response in the people in the margin on the far right.
That is, the response of those assigned to A regardless of their assignment to B.
We compare that to the response of those assigned
to not A regardless of their assignment to B.
We do a similar comparison across the margin
at the bottom for those assigned to treatment B
versus those assigned to not B. If we are indeed interested in assessing
the interaction, we have to compare the effect of A versus not A, and those
with B, to the effect of A, versus not A, and those with not B.
So we are comparing the cell responses instead of the margin responses.

#### Example: ISIS-3
An example of a factorial design is ISIS-3, that is the
International Study of Infarct Survival-3.
ISIS-3 was designed as a three by two factorial.
ISIS-3 was testing aspirin plus heparin versus aspirin alone.
Those treatments were presumed to act through an antithrombotic mechanism
on the outcomes of myocardial infarction and stroke in cardiovascular death.
ISIS-3 was also testing Streptokinase versus tPA versus APSAC, and
these three treatments were presumed to act through a fibrinolytic mechanism.
So you see in this example of ISIS-3, we had a common primary
outcome, but we had two different proposed mechanisms of action on the outcome.
This factorial design was also considered a large, simple design,
and we'll discuss this type of trial more in a moment.
There were more than 41,000 patients in ISIS-3, and it had more than 914
participating hospitals, and these hospitals were in 20 different countries.

Here is the representation of the ISIS-3 three by two factorial table.
In the top row of this table we have
the patients who are allocated to aspirin plus heparin.
In the bottom row of the table, we
have the patients that were allocated to aspirin alone.
In the first column we have patients that were allocated
to Streptokinase.
The middle column is patients allocated to TPA, and
the right most column is patients allocated to APSAC.
And the cells within the table represent the combination of the two treatments.
So in the top left cell we have patients allocated to aspirin
plus heparin and allocated to Streptokinase
and similarly for the other cells
in the table.

## Large, simple design
Large, simple designs are exactly as they sound.
* They are large, and they are simple. 
* They are characterized by a very large number of patients, usually numbering in the tens of thousands. They are recruited from many, many centers. Habing broad elgibility criteriaIn ISIS-3 we saw that they had over 900 centers,
* and they require minimal data collection on each participant.

The rationale for a large, simple trial is that it takes large sample sizes to detect modest benefit. We don't find many penicillins anymore. We don't find treatments that have effects that are so large that we can see the effect by only observing a small number of people.
If we are looking for a treatment that provides a small survival benefit for those with heart disease for instance, we need a large sample size to detect it. 

* Why would we be interested in a small clinical effect?
Well, heart disease is a very common condition, so small advancements in treatment that improve
the survival time by even a small amount can correspond to a large public health benefit.
Another premise to a large, simple design is that there are unlikely to be many treatment interactions. We aren't likely to have a well-defined subgroup of people that respond well to therapy when others don't.
* Given the assumption that it's unlikely to have treatment interactions, it's not that important to collect a lot of information about baseline characteristics and interim response
variables because we are not expecting to want to look at the treatment effect in lots
of different subgroups or examine the mechanism, because we aren't expecting a group of people to respond very differently from another group of people.
* In a large, simple design, we tolerate less precision in estimation. With such a heterogeneous group of participants and study personnel, we'll have a wide variety of people enrolled. We'll have less control over the training and standardization of administration of treatment and of outcome assessment, so we have to expect that there will be more error or increased variance in the estimation of the outcome measures. And we counter this increase in error with a large sample size.

I'm sure that you can already anticipate some of the requirements that would
be necessary in order to make it practical to implement a large, simple design.
* You have to have an easily administered intervention. Typically, this requires that the
intervention **does not require long-term adherence**. In other words, it must be something that can be administered in just one or two contacts with the participant. The treatment **shouldn't require any adjustments** to the dose or the timing of the administration,
and it also **shouldn't need any ongoing monitoring** for adverse events. These are all major limitations to the large simple designs since a large number of interventions do require one if not all three of these activities. 
* In order to use this design, we need an easily-ascertained outcome. For example, in ISIS-3, the outcomes were 35-day mortality, which was assessed by government records and other clinical events that occurred while the patient was in the hospital. 
* You must use an outcome that does not require a complicated follow-up for diagnosis.
For example, in order to diagnose Alzheimer's disease for a study that I work on, we require extensive neuro-psychological testing, detailed medical history. We do interviews with friends and family about changes in the participant's cognitive functioning. We also do lab work and imaging. It's a rather complicated outcome, and it would not be appropriate for a large simple study where you have thousands and thousands of patients for whom who have to readily assess the outcome. At most, only short-term follow-ups can be considered.
* Large, simple trials also tend to have very limited data collection at baseline because of the assumption that we've already discussed that treatment interactions are unlikely.
* And finally, for a large, simple design we need to have confidence that simple data will be persuasive enough. 

In ISIS-3, there was no baseline form to document eligibility. The randomization was completed through a phone call to a 24-hour service that was provided by the coordinating center. The treatments were conveniently packaged for ease of use, and they were already available at the recruiting centers. There was no restriction on the use
of ancillary treatments, so the treatments could be used in addition to other clinical
management that was ongoing for any particular patient. And at discharge, a one-sided form was completed to document events that had occurred during the hospital stay, and the only follow-up after discharge was through search of the government records for vital status. These are both extremely simple methods of data collection. 

## Testing for Hypotheses Other than Superiority

We're going to talk
about testing for hypotheses other than superiority.
Frequently, when we think of the hypothesis of interest
in a clinical trial, we think of the superiority hypothesis.
Is treatment A better than treatment B?
Or is treatment B better than treatment A?
In this section we'll be talking about designs
where we're testing for equivalency or non-inferiority instead
of superiority.
These are designs that can be used to
compare a new intervention to an established intervention.
When we use one of these designs we might think that treatment A is as
good as or the same as treatment
B for treating or preventing a specific condition.
But we believe that the use of treatment A might have some other kind of
benefit such as less severe adverse events,
or treatment A might be easier to administer
than treatment B, or treatment A might be cheaper than treatment B.
Another use of these designs is to do head-to-head comparisons
to two or more established treatments for a specific condition.
This uses has been discussed recently quite
a bit with respect to comparative effectiveness research.

### Equivalence Design
First, I'm going to introduce the equivalence design.
In the equivalence design, the objective is to show
that the intervention response falls
sufficiently close to the control response.
That is, we are trying to demonstrate the equivalence of the two treatments.
We could never show that the two treatments are
exactly equivalent, because that would require an infinite sample size.
* So with the equivalence design, an important question that we have to address very early
on in the design process is, how large can the difference be between two treatments,
for the treatments to be considered equivalent?
* Usually we want that detectable difference to be extremely small. We want to say that the difference in between the treatments is within a certain small margin in order to call the two treatments equivalent.If the difference that we observe is larger than the margin that we've set, we would say that these two treatments are not equivalent. So for both of these reasons, to rule out large differences and to have a large probability of detecting a difference should it exist, we need a large sample size for equivalence designs.

So as with the superiority design, the comparison that we want to
make in an equivalence design is between a null and an alternative hypothesis.
However, for an equivalence design, we flip the way
we define these two hypotheses.
* That is where for superiority design we are used to saying that the null hypothesis is that there is no difference between the treatments. For an equivalence design, we say that the null hypothesis is that there is a difference between the two treatments. And our alternative hypothesis for the equivalence design is that there's no difference between the two treatments. So then since we flipped our null and alternative hypotheses.
* We are also essentially flipping our Type I and Type II errors. So that for an equivalence design, the Type I error is to show no difference when there is one. And the Type II error is to show a difference when there isn't one.

#### Example

I pulled this example of an equivalence design from PubMed.
In this study, which was coordinated by the Jaeb center in Tampa,
the objective was to compare two treatments for the treatment of moderate
amblyopia in children ages 7 to 12 years old.
The two treatments were weakened atropine or patching
of the sound eye for two hours a day.
The investigators in this study had previously conducted
another trial where they tested the combination of
patching and atropine and they found that this
combination was effective in treating children with amblyopia.
But even after the trial, most health care providers still did not initiate
a combination therapy for children with ambyopia.

So, the investigators decided to test if the two
therapies were equivalent to one another when used individually.
The children in the study were seen for follow-up visits at 5 and 17
weeks following enrollment, and the primary outcome
was visual acuity, controlling for baseline acuity.
The study was designed to test the equivalence of patching and atropine.
The equivalence limit was five letters or one line on the ETDRS chart.
That is, the investigators felt that they should rule
out a difference of more than one line on
the ETDRS chart between the two groups in order
to call the two treatments equivalent to one another.

### Non-inferiority
The last design that we're going to talk
about in this section is the non-inferiority design.
This is another example of testing a hypothesis other than superiority.
In this case, the objective is to determine whether a
new treatment is at least as good as an established treatment.
To do this we test to see if the hypothesis that
a new treatment is worse than the established treatment can be rejected.
So our null hypothesis is that the new treatment
is worse than the established treatment, and to reject this
hypothesis, we need evidence to show that the new
treatment is at least as good as the established treatment.
You'll note that this type of statistical test is, by definition, one-sided.
In other words, the observed estimates from which we would reject the null
hypothesis, are located entirely in one tail
of the probability distribution of the outcome.

Operationally, we need to show that the
new treatment's response, if worse, is still sufficiently
close to the established treatment's response so that
we are comfortable with saying that the new
treatment is as good as, or not worse than the established treatment.
Again, like with the equivalence design, we're
looking for a very small detectable difference.
But for the non-inferiority trial, the hypothesis is one-sided,
whereas with the equivalence design, the hypothesis is two-sided.
A one-sided test does not require as much evidence to reject
the null, as a two-sided test at the same error level.
Which means that a non-inferiority design does not require
as large a sample size as the corresponding equivalence design.
But you have to keep in mind that cost of using a one-sided
test is that you're rejecting the null with a lower level of evidence.

#### Example 
An example of a non-inferiority design is the advance to trial in which Apixaban
was compared to Enoxaparin for the prevention of venous thromboembolism after
total knee replacement surgery.
Enoxaparin is a low molecular weight heparin, and is frequently
used for the prevention of
venous thromboembolism after major joint replacement.
However, Enoxaparin increases the risk of bleeding, and it can be cumbersome to use.
So the investigators proposed that Apixaban, which is an orally active
factor XA inhibitor might be as effective in preventing venous thromboembolism.
But may have a lower bleeding risk and it
might also be to easier to administer the Enoxaparin.
In advance two the patients were allocated
to receive oral Apixaban twice a day starting
12 to 24 hours after surgery or subcutaneous
injections of Enoxaparin starting 12 hours before surgery.
Both treatment groups had placebos or shams.
The treatments
were continued for 10 to 14 days after surgery.
The patients were assessed for the main outcome which was a
composite of asymptomatic and symptomatic DBT,
non-fatal pulmonary embolism, and all-cause death.
Any of these events having an onset during treatment,
or within two days of the last dose of treatment.
The study was designed to test non-inferiority.
The non-inferiority limit was set as the upper
95% confidence limit of the risk ratio of Apixaban versus Enoxaparin,
not exceeding 1.25. And the risk difference of
Apixaban minus Enoxaparin not exceeding 5.6% of the difference.

### Confidence intervals
So to reiterate, the goal of a
non-inferiority trial is to demonstrate that the
experimental treatment is not worse than the
control treatment by more than a pre-specified
small amount.
This amount is non, is the non-inferiority margin.

we're going to look at
how non-inferiority margins are used with confidence intervals.

A point estimate is a single
value that estimates some population parameter based on our sample data.
An example is the sample mean, which is the average of the values in our sample.
And it's frequently used to estimate the unknown population mean.
An interval estimate specifies a range within which the
population parameter is estimated to lie based on the sample.
How likely the interval is to contain
the parameter is determined by the confidence level.
And that's usually expressed as a percentage.
The most commonly used confidence interval is the 95% confidence interval.
For a 95% confidence interval, one can expect
that if you sample repeatedly from the same
population, 95% of the confidence intervals of the
sample mean will contain the population mean of interest.

In this figure, we have several confidence intervals,
and these are indicated by the blue and
red horizontal lines.
The point estimates are designated with the short vertical lines
that you see in the middle of the confidence intervals.

These point estimates represent the sample estimate of the
treatment difference between the experimental and the control groups.
The solid black vertical line that runs from the top to
the bottom, and ends above the zero, is the zero line.
And point estimates that are close to the zero line indicate
that our best estimate is that there is not
much difference between the treatment effects in the two groups.

Point estimates that fall to the left of
the solid zero line, favor the experimental treatment,
and point estimates that fall to the right
of the zero line favor the controlled treatment.

In this figure, in order to show superiority of the experimental treatment,
we need the 95% confidence interval to fall entirely to the left of
the zero line.

You'll notice that there's another long vertical line that
is dashed and has a delta at the top.
This is our non-inferiority margin.
If the confidence interval crosses or falls
to the right of the non-inferiority margin,
then we cannot reject the null that the experimental is worse than the control.

However, if the 95% confidence interval falls entirely to the
left of the delta line, we can reject the hypothesis that
the experimental treatment is worse than or inferior to the control.

So in this figure, the confidence intervals that are shaded
in blue fall entirely to the left of the delta line.
So in those cases we can say that we have shown non-inferiority.
The confidence intervals shaded in red, cross the
delta line so we cannot say that we've
shown non-inferiority.
Only the bottom confidence interval falls to the left of the
non-inferiority line and also entirely to the left of the zero line.
And so, in that scenario, we can say that
we've shown non-inferiority, and we have also shown superiority.

Trials are sometimes designed with
nested non-inferiority and superiority hypotheses.
Investigators might
design the trial so that if non-inferiority is established when the
study is finished, then they can go on and test for superiority.

The more common situation is when investigators fail to show
superiority, but they might then test if they can show non-inferiority.
So they can't say that the experimental treatment is better than the control,
but they can say that it's at least not inferior by some small amount.
In the example of the non-inferiority trial
from the previous slide, the advance two trial,
the investigators had planned a priori to
test for superiority once they had established non-inferiority.

## Adaptive Design

Up until this point in the lecture we've
discussed why it could be called fixed design.
And for fixed designs the design characteristics and
features such as the sample size, and the
hypotheses of interest, the outcomes, and the treatment
groups, those are all established before the trial starts.
That's not to say that there are never changes to a trial
with a fixed design once a trial starts.
In fact, changes do have to be made
because of information that you learn during the trial,
either from data within the trial or from relevant
data that is learned from outside of the trial.
However, some trials are designed to change
depending on what's observed in the trial.
And these are called adaptive designs.
And in this section we're going to cover some features of adaptive designs.

There's been great interest recently, particularly in the pharmaceutical
industry, in clinical trials that are designed with adaptive features.
Because by adapting to what's happening in a trial they have the potential to be
more efficient and more likely to demonstrate an
effect of a drug if there is one.

To respond to this interest, the FDA has begun drafting
a guidance document for industry on the use of adaptive trials.
The FDA has started drafting
a guidance document for industry on the use of
adaptive trials in the drug development process so that
the adaptive trials can be used as part of
the new drug application for submission to the FDA.
In the draft of the guidance from February of
To respond to this interest, the FDA has begun drafting
a guidance document for industry on the use of adaptive trials.
The FDA has started drafting
a guidance document for industry on the use of
adaptive trials in the drug development process so that
the adaptive trials can be used as part of
the new drug application for submission to the FDA.
2010, the FDA defines an adaptive trial as a
study that includes prospectively planned opportunity for modification of
one or more specified aspects of the study design and
hypotheses based on the analysis of data,
usually interim data, from subjects in the study.

It's important to note that, in the context of adaptive designs, we're
talking about **adaptations that are planned and detailed before data are examined**.
This is an important distinction, since, as I mentioned before, changes
in steady procedures occur in more traditional, fixed designs, as well.
But these changes are as a response to what's
seen after examining the data.
However, when we speak of adaptive designs, we
are talking about designs that have pre-specified design adaptations.

There are many potential adaptations in adaptive designs.
  * Investigators can change randomization probabilities. For instance, one might calculate the probability of success or improvement in the outcome for the different treatment groups continually as participants move through the trial. And then this information can be used to adjust the probability of being assigned to the different treatment groups so that the next participant has a higher probability of being assigned to the treatment group that's showing a higher probability of success. 
  * Another adaptation is a change in the sample size based on the accruing data. Group sequential methods, which are methods of stopping early due to benefit or harm of a treatment, and also methods for stopping early for futility have been around for some time. And they are some of the most common and well understood design adaptations.
  * Also investigators might be unsure about the best visit schedule for observing outcomes.
So they could specify a change if it was discovered that the length of follow-up was unnecessarily long or if they discovered that the length of follow-up should be longer and the investigators could also increase or decrease the number of interim follow-up visits.
  * You can also change the treatment groups during a trial. In order to do this, you need to specify rules under which a new treatment can be added, or rules for when a current treatment can be dropped. You might want to change the dose or duration of one or more of the treatments. And you might decide to change the list of allowed or required concomitant meds.
  * Investigators sometimes pre-specify that they may change from non-inferiority to
superiority or vice versa during the course of the trial. And it's also possible to even change the eligibility criteria. For instance, if one subgroup is responding to treatment and another subgroup is not, there are enrichment designs where by you continued recruitment
only in the subgroup that is performing well. Investigators also may choose to change their outcome measures or their methods of analyzing their outcomes. 

As I mentioned, calling a trial an adaptive trial does not mean that any change is allowed at any point in the conduct of the trial. There are certain principles that one should
follow in the design of an adaptive trial. 
   * First, the adaptation trigger should be explicitly stated in the protocol. 
   * Second, the adaptation itself should be detailed. For instance, the investigators may want to change the sample size during the trial. When we do our original sample size calculations we have to make lot of assumptions based on preliminary data or data from the literature or sometimes even just from opinion of the investigators. We have to make these assumptions in order to estimate the sample size that we need to detect a given difference with a certain power. An adaptation that might be pre-specified in a protocol would be that
after say 50 patients have been enrolled, the investigators will reevaluate the sample size calculations based on the data that have been seen so far and they can adjust the sample sizes needed to maintain their original power. Or the investigators could enroll from two sub
groups until a certain point in the trial at which they could compare the treatment response rates in the two groups. If they find that the response rate was say three times as large in one group as compared to the others the investigators might choose to continue recruitment only in the sub group that's responding better to treatment. 

Many of you have probably submitted a protocol to an institutional review board or IRB, and so you're probably familiar with the IRB amendment process. In most trials with a fixed design, investigators have to submit amendments to the IRB in order to make design changes like the ones that we've talked about in the past two slides. However, in an adaptive design the IRB
submission and approval includes the planned adaptations. So amendments are not required for adaptations that occur as planned. However if there's other changes that are needed, that were not described in the original approval, those would have to go back through the IRB for review and approval. 

 * So the advantage of an adaptive design is that it's more flexible than a fixed
design insomuch as that design allows for changes within the specified list of possible adaptations.
 * Adaptive designs have the potential to be more efficient than a traditional fixed design.
And note that I say they have the potential to be more efficient, but they aren't necessarily so. 
 * And it's also true that some adaptations increase the likelihood of showing a treatment effect if there is one. 

There are also several limitations of adaptive designs. 
  * When investigators change design parameters partway through a trial, it can be difficult to interpret the overall treatment effect estimates. For instance, using an example from the last slide, if at the beginning of the trial we recruit from two sub-populations and then we adapt, after an interim analysis, to recruit only from the better responding sub-population, what is our treatment effect estimate? Do we exclude participants in the poor performing sub-population? How do we interpret our effect estimate? 
  * Also another disadvantage is that it has been shown many times that treatment effect estimates at interim analyses can be meaningfully different than the estimates that we get at the end of the trial. This lack of reliability of interim estimates can have  onsiderable unfavorable consequences for adaptive procedures. 
  * An adaptive procedure that permits design changes such as an increase in the targeted number of study events based on interim data about treatment efficacy allows at least indirect information about efficacy and safety results to the investigators and to the sponsors and other people outside of the data monitoring committee even if the adapted
procedures implemented by the data monitoring committee. Because the investigators know that the change has occurred.
  * Finally, from a practical standpoint, adaptive methods can be hard to implement if the
methods require quick access to data to say change randomization probabilities. This can be a problem since there's typically at least some delay in the measurement of the data and the availability of the data for analysis due to data entry lags or lags in data editing. And these adaptive protocols require extensive documentation of potential triggers and adaptations.


#### Example
Our final example of this lecture is of an adaptive design.
The I-SPY 2 study is a collaborative
effort of academic, government and pharmaceutical groups
under the auspices of the foundation for
the National Institutes of Health Biomarkers Consortium.
In I - SPY 2, the investigators are comparing the efficacy of novel
drugs in combination with standard chemotherapy.
They are comparing novel drugs in combination with standard chemotherapy to
standard therapy alone, for the treatment of locally advanced breast cancer.

The objective of I-Spy 2, is to identify and
improved treatment regimens based on the biomarker signatures of disease.

In I-Spy 2 there are two arms of standard chemotherapy,
plus five additional arms of a new
experimental drug added to the standard therapy.
Each experimental drug is tested in the minimum
of 20 patients and a maximum of 120 patients.
And after 12 weeks tumor tissue is collected surgically to assess whether or
not the patient has a pathological complete
response, which is the primary outcome measure.
Regiments that have a high probability are being more effective than
standard therapy within some biomarker signature graduate from the trial.
And regiments that show a low probability of
improved efficacy with any biomarker signature will be dropped.
So new experimental drugs can enter the trial as others graduate or are dropped.
I-SPY 2 also uses adaptive randomization.
Drugs that do well within a specific biomarker signature are preferentially
assigned within that signature.
Candidate drugs for I-SPY 2 have been tested
and found safe in phase one clinical studies.
And they also have preliminary evidence of efficacy
for breast cancer from preclinical or clinical studies.
There's an independent group of experts that determines the list of
new drugs that would be contenders for inclusion in this study.
