# Outcomes

##  Definitions & Types

First thing that I want to define is that I am using the term outcome but that
term is synonymous with the term endpoint which is more commonly used
in the literature than outcome. The reason that.
I use outcome, as in my training it was stressed that you always continue to
follow patients for the planned duration of the patient's follow-up.
And that may be longer than when they have an endpoint.

  * So an outcome doesn't imply that you're going to stop.
Follow up, so, in our group, we encourage people to use
the term outcome. Now, an outcome is usually a quantitative
measure that somehow allows us to evaluate a treatment.
And it puts some metric on how effective a treatment is, and so usually clinical
notes or peoples subjective opinions are not very useful for this purpose.
Its hard to
do statistical analysis on those type of things.
So Usually, we try to use the definition of the outcome
to make what may be a qualitative thing a quantitative thing.
  * It's important that an outcome reflects the objectives of the trial.
You may choose different Outcomes based on whether
you're looking at an efficacy versus an effectiveness trial.
Whether your primary motivation
is to evaluate the safety of a therapy or different therapies.
Some clinical trials evaluate the process of treatment, or the
process of an intervention in a broader sense, or the cost.
  * Usually outcomes are determined in each study participant or
we would say in each study of randomization unit.
And the objectives of the trial are
met by aggregating those outcomes across participants or
randomization units by treatment.

So there's typically hierarchy of outcomes in a clinical
trial because once you start to put together all
of the resources needed to conduct clinical trials, you're
probably going to to want to measure more than one thing.
  * So there's a primary outcome that should reflect the
overall objectives of the trial and whatever is stated
as the.
The primary hypothesis and we'll go through some examples in a few slides.
And that's the design variable and what I mean by the design
variable, that's the variable for which the sample size is calculated on, usually.
And it is related to the stage or type of research you're doing.
So, what I mean by that is in, in preliminary phase one studies,
the outcome may be a safety Related at any adverse events.
In later stages of research the outcome may be a blood level, such
as cholesterol, and then in another stage it may actually be a clinical outcome.
So it's the type of research that's being done.
  * Secondary outcomes are measured to
evaluate other potentially important treatment effects.
Certainly
They could be adverse events or toxic effects of
the treatment and usually if you want to do that
well, in the context of a trial and you
have some sense of a agent's toxicity, you would.
Tried to make sure you had appropriate ways to measure that.
For example, if you're doing a trial of a steroid regimen
versus a steroid sparing regimen, you probably want to institute into the
trial some measures of steroid toxicity or other
toxicities of the new treatment, so that you can really Evaluate the treatments in
terms of safety. You may be also evaluating the mechanism
of effect, this is very common I think in even large scale trials funded through
NIH there is usually some effect of effort to incorporate Some
mech, mechanistic hypotheses into the trials as ancillary studies
or sub-studies, and these may be quite well-defined, secondary outcomes.
  * And having other outcomes besides the primary outcome allows for
more complete evaluation of the treatment, for, for example the risk/benefit
ratio, or the cost effectiveness of a treatment And, beyond
the primary and secondary outcomes, there's a third layer of outcomes
that may include data related to the patient health or study participation
that you want to measure, sort of measure a process of the trial.
Their compliance with the drug treatment, drug levels, and you
may have some exploratory hypotheses that you may collect data
on without a lot of prior research, but some thoughts
as to why this outcome may be influenced by the treatments.
But this is just an
exploratory preliminary look at that.

So for that most important outcome, the primary outcome, what are the criteria?
  * Well certainly, you have to have defined the primary outcome to a great level
of detail including what it is and how you are going to measure it.
And even how you're going to analyze it. Before you start the trial.
It should not be based on data that is
developed during the trial.
Now it is possible that you'll have a primary outcome and
maybe not have a complete analysis plan when you start the trial.
But before you look at the data that, that has been collected, you want to have
things clearly defined so that you won't be
influenced by the data in sort of Inappropriate ways.
  * It should be a relevant outcome and likely to be influenced by the treatments.
  * You have to be able to measure
it accurately and reliably in all study participants.
So you don't want to take an invalidated new cutting edge
technology to use as an outcome in a clinical trial.
It has to be a proven outcome.
  * That, you're able to evaluate in everyone.
  * The assessment should be able to be done, independent of treatment assignment.
That you can
measure it in both treatment groups and it's through the same procedures.
And, it may seem, when would that not be true.
But, if you had a surgery trial vs a systemic treatment trial.
Your primary outcome shouldn't Be something very specific to
the surgery that's not going to happen in the other group.
  * And then you have to think about the
power considerations and by power, I'm really referring
to the size of the clinical trial, and the
primary outcome really is a key component in driving that.
And so The kind of nature of the outcome at its very ability.
Its standard deviation.
The frequency that you would expect to observe.
And, what do you think is a likely difference between the treatment groups.
All are important for the power calculations.

#### examples
If you have
a trial of asthma treatment, and the objective was to evaluate for
asthma control, well, there's a number of possible outcomes you could have.
You could have something like exhaled nitric
oxide, which may be a measure of inflammation,
and you might think if that was lower That the treatment was a good treatment.
You could have lung functions such as FEV1 or
peak flow, where you, people actually get on a spirometer,
a machine that you breathe into, to measure how well their lungs are working.
Or you may have something more related to how a person is actually feeling.
Their symptoms.
Are they wheezing, do they have night awakenings?
And someone with poor lung function could not have many
symptoms, so the symptoms is sort of a higher level outcome.
And also, commonly at least in asthma trials, there may be **composite
measures** you use.
That's sort of an **exacerbation**, a lack of control
of asthma, maybe some combination of a drop in lung
function Increasing symptoms, night awakenings, those kind of symptoms
and the need for medical care, hospitalizations or ER visits.
Or sometimes they use **symptom indices** which are scores that
kind of again blend together on few different facets of asthma
control-like symptoms and medication use.

Another example just to think about, is if you
were evaluating a perioperative procedure that the objective of this
procedure was to reduce perioperative morbidity so may be its
a different kind of suture or different care for wound.
If you were going to be [INAUDIBLE] finding
this, what are the considerations for the outcome?
Well, sort of the time window.
How long is postoperative?
When are you going to consider something related to the operation, versus,
well, it is no longer likely to be affected by that.
And what are the specific events that you would consider an outcome?
What type of adverse events are you looking for?
And what are the procedures you're going to use to establish the outcome?
So are you going to be interviewing patients?
Are you going to just be going back to the surgeon's office and looking
at the follow up records? And the Specific metric.
Is it going to be any adverse event and are
you going to evaluate the severity if it's too reduce morbidity?
Does it have to be a defined infection that can be assigned to a particular
organism, or is it Fever of unknown
origin's going to be considered a perioperative adverse event.
So those were just two examples to kind of put some concreteness
around what we've been talking about.


#### Metrics for events
So what are the metrics used for evaluating outcomes when you have events
as outcomes, and what I mean by an event is an adverse event.
Deaths, heart attacks.
There are a few different types of ways of evaluating an event.
  * And for the most part events are either they happened or didn't.
So its the presence or absence of
something or whether its normal or abnormal.
Did the person die or didn't they? .
Sometimes, you can transform data that is continuous by nature, something
like hemoglobin levels into an event by saying did they have anemia or not.
So by just instituting a cutoff value.
And if you just look at whether an event occurred Or didn't
occur, you would consider that a
dichotomous outcome, if the defined outcome is,
did a person have a remission at six months, or not?
So you want to actually measure something at a specific time usually.
But without relation to time in a, in
another sense, its just did it occur or not?
  * So you can take that one step farther and add the dimension of time to
a dichotomous outcome, and that's usually a
time to event type analysis or survival analysis.
And, that usually
Will be more powerful for detecting treatment
differences than just a straight dichotomous outcome.
And it also allows for accounting of, of people
who you didn't observe the outcome in who are censored.
So that would be sort of the second level of looking at events as outcomes.
And actually survival events are very typical In clinical trials.
  * Or you could look at one event, but allow for repeats.
So, you could look at rates of events
between treatment groups, so it's a one, zero event.
Did they have an asthma exacerbation or not, but you follow them for
a period of time and a particular patient could have more than one event.
Or it could be a fever or any number of type of events.
In that situation, you need to have the follow up time because you're going to get
If you're going to calculate a rate it's usually in events per person year.
So you have to be sure to be following them
for how long they're being followed before and after these events.
And you tend to analyze the rates of events.
One important caveat in type of analyses is
that usually events within people, or within randomization units,
may not be independent.
So they may not Follow kind of the strict rules for analysis so you need to
account for that non-independence in the analysis, and
there are plenty of procedures to do that.
  * And finally, you can look at a number of events and say of one of these
happened, then The outcome was achieved. I gave you a example of that
in the asthma treatment before that it could be, did the person have an ER visit?
Did they need an increase in their medication or miswork?
So its two or more events that are related to the
disease process or would be considered appropriate for evaluating the treatment.
And in that case, you know you could consider that
composite measure to be either dichotomous measure or time to
event or a rate.
It could be any of those three that we've gone through before.
All it's adding to the definition of the outcome is that it could be two
relatively kind of disparate things, like increase in
medication use versus a drop in lung function.
It could be one or other of those things that kind
of measure sort of different things, but all are related to asthma.
  * So the other type of measure
that you can have is a continuous variable.
Something like cholesterol levels in the blood.
And, in that case, the outcome is usually either the value, compare the cholesterol
levels by treatment across the groups Or compare the change from baseline.
So you may have a baseline level on everyone
and then after you institute treatment, how did that change?
There's usually some standard units for
that, like for laboratory dye use or Some sort of continuous
metric, like a score, that may have a range of reasonable units.
You need to define an important difference between the treatment
groups that you would consider clinically important, if you're talking
about a pharmacological trial, that is not something that you
could get a small Difference between two measures and considered them
to be statistically significant but that difference
may have no real consequences on broader measures.
One thing with continuous variables, again, you can measure them repeatedly
over time, and use those measurements either to project a slope
of how things changed over time or if you think there'll
be an, an initial change and then no change thereafter maybe.
Just a better estimate of what the change is.
Typically continuous outcomes are more powerful than discrete outcomes that
you need smaller numbers of people to do your clinical trial.
But it really hinges on what you think the
important detectable difference between, let's say the mean values in
each group, so the mean value of cholesterol in
one group versus the mean value in the other group.
So if you want to detect a small difference
then you'll need more people.
The fact that you can kind of
calibrate the differences in the continuous case where
you can say I detect a small difference or have it be a larger difference you
have a little more ability to influence
the sample size whereas in a dichotomous outcome
where it's yes or no it's a little
more difficult to justify differences in sample sizes.
And usually, just because of the nature of the beast
those type of trials with a dichotomous outcome require more patience.
But it's important, when you're looking at
continuous variables, to be checking the distributional assumptions.
Because you, you know, you have to make sure if
you're doing the power calculations based on a normal distribution.
That the outcome really follows that.
  * And, sort of a subset of continuous outcomes is
an ordinal scale, where there are categories, kind of
qualitative categories that are ranked maybe 1 to 5
or A to D This is commonly used in
adverse event grading, that it's grade one to grade five.
But the differences between these categories
are usually qualitative and not necessarily
equal across the categories so they're sort of summarized in this ordinal ranking
scale that creates some order in them. But isn't necessarily that if you're
a two That is twice as bad as a one, and a four is four times as bad as a one.
That relationship is, it's not necessarily linear in that sense.
But they can sometimes be used in clinical trials, and they
can allow for grading between you know, complete remission, partial remission, no
remission.
And so allow for a little more flexibility in the outcome 

And finally I just want to briefly go over objective versus subjective outcomes.
And really it's a spectrum of objectivity to
having a very objective outcome to a subjective one.
  * And there's very few outcomes that are absolutely objective that.
Can't be influenced, sort of, by the observer, or the person reporting
the outcome. And, that would be total mortality.
People are usually dead or not.
The evaluent later's bias can't really be introduced there.
So Objective outcomes tend to be
clinical events or measurements that are pretty definitive.
That require little or no subjective judgments.
And they may require some, like what is a myocardial infarct?
But you can apply rigorous definitions to limit
the interpretation of the grader.
Of the outcome, you could have infections with
confirmed cultures, they were culture positive as well.
And, so, you can make things that are
pretty well standardized and as long as you Have
a very clear protocol for how they'll be
evaluated, you'll encourage it to be an objective outcome.
  * Subjective ones of, ones that rely
more on judgement and kind of gastolve that.
Can't really be quantified quite as well.
One is a Karnofsky score, which, if you're not familiar with,
its just a score from 0 to 10, that sort of
tries to incorporate how patients are doing with daily living skills
and, and 0 is dead, and 10 is you're having no problems.
But even when you get to evaluations of slides and
things, or tests, like CT scans, they're sort of
What is the bias that is brought by the evaluator.
And, so, if you have a more subjective outcome, that makes masking more
important so that whoever's evaluating the outcome,
won't be influenced by the treatment effect.

Ways to enhance accuracy and objectivity are to **clearly defined criteria
for evaluation**.
Make sure you are using a **validated measure or do some pre-testing**
of that to train the people who are going to be evaluating the outcome.
And to make sure they're all using the same criteria the same way.
So you can have test cases so that, to
see what the agreement is across evaluators and standards.
And these people may, you know, be operating at different clinics.
So it's not like they're going to evaluate the
same patients over the course of the trial.
So what you're trying to do is to have them **evaluate the same case to train** them.
You can have a **panel of assessors** to have
more than one person do the assessment, and have
them operate independently and as they come back with
different results then you would have some method for adjudication.
And you certainly **ongoing quality assurance** throughout the
trial to make sure there isn't a drift in how things are evaluated.

And finally just to comment on patient's opinions which
are generally subjective, but are
increasingly important in evaluating treatments.
How someone's doing, their health status or their change in status.
You can imagine that for these type of outcomes
where you have to rely on the patient's assessment
of how they're feeling or what is happened to them over the past interval.
That masking is even more important.
So that they are not influenced by their knowledge of a treatment assignment.
And one thing that is important to recognize is the effect of being studied.
And once a patient is in a trial and has some more attention focused on them.
There's usually a positive effect of that.
Paying attention to people in
a positive way, which we would hope would be
true in a clinical trial, will usually have a positive
effect on people, that they may be feeling better because
you've taken the time to ask them how they're feeling.
These type of data are difficult to quantify.
They typically use scales of, you know, how are you feeling On a scale
of 1 to 10, but can be a challenge to get really accurate data.


## Influence on Design

how the choice of outcomes may influence the design of a trial.

### Efficacy vs. Effeciveness

* Efficacy trials are usually those that are looking to be
able to evaluate the condition under the best of all circumstances.
Does this intervention change outcomes
given sort of every benefit
* effectiveness, which is looking
how the intervention will work in a more real world setting.

So you can imagine that this could reflect the choice of outcome.
In a vaccine trial, if you were looking at effectiveness
you might go to a larger population and look for all clinical cases of influenza.
And may or may not be confirmed by laboratory evaluations.
In a efficacy evaluation, if you were
trying to evaluate whether this vaccine seem to
effect and promote immunity, you might base the
first studies on laboratory confirmation of antibody level.
If you were looking at asthma trial an
effectiveness trial might look at actual things that happen to patients.
Did you decrease hospitalizations or oral steroid courses?
In an efficacy trial where you' were trying to
see, well, does this treatment have a positive effect
on some of the clinical measures of disease, you
might look at FEV1, which is forced expiratory volume
in one second.
So really quite different outcomes and you'll need quite different sample size.


### Choice of outcome is crucial

The choice of the outcome is, is crucial
to so many aspects of a clinical's trials design.
* It's usually based on prior research and also, you have to
look at prior research to make sure that your study, with your results and your
outcome, will be able to be integrated
into the body of evidence about a treatment.
It's important to know what other people measured to assure
that your study can be evaluated in a larger context.
Not necessarily mean that you have to copy people,
but perhaps you may add another secondary outcome that reflects
what others have used but may chose another primary outcome, that
you may think is a better choice, for a variety of reasons.
* But it is a key factor for determining what the **eligibility criteria**
are for a trial, the **sample size**, how long are you going to need to **follow
people** to get changes in this outcome, or to have events occur?
And what's the **frequency**? Do you
need to see people frequently so you don't miss things, or
is it something that people would remember happened in terms of events
like an MI that, you know, maybe you don't have to
follow up with them that frequently, at least with active study visits.
Also, the need for **masking**.
The more subjective the outcome, the more that
either the patient or the evaluators can influence
it, the greater the need for masking, and the type of masking, whether
it's just the patient that's masked or the patient and the clinician that's masked.
So all of this is influenced directly by the outcome that's chosen.
* Of course the personnel and resources required for the trial.
You may need to have more sophisticated resources and
more highly trained personnel if you're going to use some imaging
procedures as an outcome.
* It certainly will influence the quality assurance procedures that are
put in place to ensure the accuracy of outcome assessment.
Whether that's going to involve site visits or can be done from afar, maybe by
a panel of adjudicators or you know, reviewing samples from
data collected during the study. 
* It influences the cost of the trial  
* it
may also influence the generalizability of results.
How quickly could these results, or
how meaningful are they, for clinical practice?

### Considerations: Three Bs
So kind of in summary of those considerations is the three B's.
* How does the outcome related to the **biology** of the
disease or condition that you're trying to change with the interventions?
And does a change in the outcome reflect a really clinically relevant factor change?
And the closer, usually, you get to clinical
relevancy, the bigger the trial needs to be.
* But certainly, **biostatistics** in the power calculations.
What's the detectable difference between the
groups, that is, both plausible and practical?
So what can actually, hypothosize to happen,
without being wildly optimistic and is that a practical thing to be able to do?
* And the most important aspect of practical is usually the **budget**.
Can you afford the total N for the trial,
the total sample size that a particular outcome will require?
And can you afford to measure it?
Because reliably in every participant, do you have that equipment
at all the centers, and can it be done in everyone?
Or will participants refuse to have the measurement?

So the choice of an outcome is usually a balance of these three.


### Example - Choicesfor HIV trial

So, if the trial is for evaluation of an antiretroviral treatment there
are several measures that might be used. 
1. You could measure survival.
Did the person, are they dead or alive? Did they progress to AIDS or not?
2. You can look at the immunologic response, in the person.
You could see what their CD4 counts were or other measures of
how they're responding to H, the HIV disease, the virology response.
3. It's very common to measure HIV viral load levels in trials.
Or you could, you could be looking at the change in a patient's status.
So, hows a patient?
4. What's the quality of life?
Because you may have a great immunologic and virologic response, but if they
can't get out of bed, then that is not necessarily a good treatment.
5. You may look at specific toxicities if there's a concern about that.
And certainly in a lot of the
more recent antiretroviral therapies there have been concerns
about the effects of long-term toxicities that
relate to cardiovascular disease, like elevated triglycerides.
6. Or you may be just looking for other
side effects, that could be one of the outcomes.

So you can see for a particular treatment there could be a range of outcomes.

And the choice of the primary outcome will depend on the objectives or
the stage of the research.
* when you're in a phase one situation where the emphasis is on safety the
outcomes that will be most important have to do with toxicity, specified or, or not. (5-6)
* In a phase two study, where you're looking at short-term efficacy
outcomes, you may be looking more for laboratory kind of surrogate outcomes like
immunologic or virologic response to the treatment. (2-3)
* If you were designing a pivotal trial that you were going to
use to bring this to market, normally what we think of as
a phase three trial, you'd probably be more interested in long-term efficacy
measures, such as survival and also the toxicities associated with the drug. (1-5)
* And then in a phase four trial, which is usually a post-marketing
trial, you may be comparing two drugs to see which one
is better, or two regimens, and in that you're probably interested in
a lot of different measures: the survival, how the patient feels on
the treatment, what their quality of life status is and the toxicities. (1,4,5,6)
And you may indeed be also interested in the influence
of the treatments on short term measures of efficacy such
as viral load and CD4 counts.
So, regardless of the choice for a
primary outcome, you'll probably in many situations
collect data on other outcomes even when they aren't specified as the primary one.

### Features to protect outcome
So how do we design trials to protect the primary outcome,
to make sure were getting an accurate, reliable unbiased assessment of outcomes?
* Well, one thing, we protect against bias by randomizing people
to different treatment groups, so that there isn't a propensity
for one type of person to be put in a
particular treatment group that may be related to the outcome.
For example, putting the sicker people with the new exciting treatment.
* We also define the primary outcome,
specifically in the protocol, and try to avoid
any ad hoc definitions about what the outcome is.
That should make you suspicious, if someone's trying to
change the definition of the outcome, or what's included
in the outcomes after all of the data have
been collected, and maybe started to be looked at.
So you don't want to do any on the spot
definitions or post hoc selections of the primary outcome.
That should be defined well before the data
are collected and certainly before they are analyzed.
* You want to have standard methods for the measurement, and
you want a measure that at a standard interval.
So you want to have not only methods for
the measurement but also a standard follow-up schedule
so that you can avoid differential follow-up in
the design of the trial or by default.
And so what I mean in the design of the trial, if you
are doing a surgery versus medical treatment trial,
you may need to see those surgery patients
more often, right after surgery, than you would
need to see the patients assigned to medical treatment.
But, in that case, you could either design a trial with a fair amount of follow-up in
the beginning for both groups or, you could
very clearly define what is clinical care and what's
study follow up and make sure that you keep them separate and not start
doing study measurements at a follow up that is really just for clinical care.
* So another feature we use to protect outcomes is masking of patients,
clinicians, and/or the people evaluating the outcomes to the treatment.
So they don't know when they're evaluating outcomes
what treatment the patient is assigned to.
* You would try to limit observer variation and that's within an observer.
So there is adrift over time so we can have
some quality assurance that the person evaluating things isn't changing
how they evaluate things over time and also to make
sure that different observers have the same criteria for evaluation.
* and it's also important that you analyse
all the events from all the patients enrolled.
And when you start seeing publications that
talk about evaluable patients, then that should
raise a red flag, because why did they exclude events that happened in patients?
And that could mean that they're trying to manipulate or,
either purposefully or not, what is the overall interpretation if
there's a specific type of patient outcome that's not included?
So, we're still protecting the outcome in the analysis by using
the intention to treat analysis.

# Analysis issues in clinical trials

## Analysis by Assigned Treatment (Intention to Treat)

## Purpose of randomisation

As a reminder, we use randomization to allocate
treatments because it protects us from selection bias.
And that protection is the primary reason that the
randomized trial is considered the gold standard of study design.
And so our analyses,
in order to capitalize on the benefits of
randomization, must be based on the randomized assigned treatment.

So randomization assigns people to a treatment group, but in practice some
people will not actually get any or some of their assigned treatment.

* And this is an example from the National Emphysema Treatment Trial or NETT.
In this figure we show what happens with so-called cross-overs.
And here I'm not talking
about design cross-overs that we discussed in
our design lecture, but I'm talking about
undesign cross-overs or cross-overs that are mostly
out of the control of the investigators.
Participants were assigned to receive, either lung volume
reduction surgery, or the standard of care medical management.
The potential participants went through a
lengthy run in process before randomization
to try to screen out people who were likely to be non-adherent.
But still after randomization, some people who were assigned
to the surgery group refused to have the surgery.
And so what they actually received was the standard of care medical management.
And some people who were randomized to receive medical management, decided to
pay their own money, and seek the surgery outside of the trial.
So we had some people randomized to
surgery, who received medical management, and some
people who were randomized to medical
management, who actually received the surgery.

* Another example from the Alzheimer's
disease anti inflammatory prevention trail, or
ADAPT, is what can happen when there's none adherence during follow up.
This example is not necessarily a cross over immediately after randomization,
but here the non adherence occurs during some point in follow up.
So in ADAPT, the participants were randomized
to receive either an NSAID or a placebo.
And the participants had to take these medications
for a long period of time, for years, actually.
So during the process of follow-up, there were
some people assigned to the NSAID group who decided
that they couldn't tolerate the side effects of the
NSAID, and they decided to no longer take it.
There were also, a subgroup of people in
the placebo group who began to take an NSAID
on their own, during the trial.
For example, some of them received a
prescription for an NSAID to treat their arthritis.
So In ADAPT, we had some people assigned to the NSAID
group who at some point during the trial began to receive
no treatment and some people assigned to the placebo group who
at some point during the trial began to take an NSAID.

## Intention to treat (ITT)
So the question is, what do we do about these unplanned crossovers
and the treatment non-adherence when we analyze the results from the trial?
* And the intention to treat philosophy tells us,
that we analyze data based purely on randomization.
* So this means we ignore **ineligibility**, and those are
people who were enrolled although they were not eligible.
We ignore **complete nonadherence**, like the example we just saw from NETT
where some people received none of their assigned treatment.
We also ignore **treatment terminations** and
**treatment switches**, where people receive the assigned
treatment for a while, and then they
terminate or switch to a different treatment.
And we ignore **partial adherence,** which occurs in a lot of people.
A lot of people will take some portion but not all of their prescribed treatment.

### Violation of ITT
And this may sound illogical,
but in principle it isn't.
So to explain why it's not illogical, I'm going to
talk about what would happen if we did not use ITT.

So we worked hard to make this an experimental design to
avoid the problems of self-selection of treatment and confounding by indication.
But if we analyze according to the treatment
received, or adjusting for adherence, then we are allowing
these biases to creep back in, so we don't know all the reasons
for non adherence but what we do know is that it's frequently not random.

A lot of times, treatment adherence is related to the
treatment, because some treatments are harder to adhere to than others.
And it may also be related to the outcome, because people who do not adhere,
may differ from those who do, with respect to factors that relate to the prognosis.
For example, people who don't adhere might have more severe disease. Nonadherence is not random.

### ITT properties

* ITT is unbiased
So we use randomization to prevent bias, but it
only does this if we analyze by the treatment assigned.
And in a perfect world everyone would take a
100% of their assigned medicine 100% of the time,
and what we'd be able to estimate with a
clinical trial would be this theoretical notion of true efficacy.
So the effect of the treatment with perfect compliance.

But we can't guarantee that a person will take the treatment as it's assigned.
And in fact, the right to individual autonomy
is one of our basic principles of medical ethics.
So, in the real world, outside of the trial, some
people will also not take their treatment as it is prescribed.

* ITT measures effect in global sense
And so we can think of the clinical trial instead of being
a test of the treatment received, we can think of it as being
a test of a treatment policy or a treatment prescription.
And in this way it is like real life and it is a generalizable result.
Although it is likely a conservative estimate
of the true efficacy with perfect adherence.
Another nice feature of an institrite analysis is
that they're clear and simple to declare up front.
They're not post hoc.

* ITT analysys is not post hoc
So in clinical trials we try very hard to avoid changing
our methods after we've seen the data.

And adherence is something that's hard to quantify before you've seen the data.
So to include adherence in our primary analysis
plan would be difficult to specify ahead of time.

And it may be fine to include some measure of adherence
in our secondary analysis but it's not appropriate for the primary analysis.

* ITT analysis require ITT data collection philosophy
Finally,
analyzing data according to the intention to treat philosophy requires
that you collect data according to the intention to treat philosophy.
And what I mean by this is that all data are
collected for people once they're
randomized, regardless of their treatment adherence.

So once you randomize a person, you follow your visit schedule, and you collect
all outcome assessments, even if they never
take the first dose of their study medication.

And if you do your data collection in this way, you can do the intention to treat
primary analysis, but you can also do other analyses,
a secondary analysis such as analysis by treatment received.
Or the analysis on some subgroup of
participants that you think are good adherers.

However, the reverse is not true.
If you collect your data only on people as
long as they're compliant with their study medication then
you lose the ability to be able to perform
a true intention to treat analysis at analysis time.

So just to reiterate, intention to treat analyses
require an intention to treat data collection philosophy.
And the data collected according to intention to treat
are more complete and allow you more options for analysis.

### Anayses of adherence

So what do we do with adherence?
We think that treatment is associated with adherence, and
we also think that adherence is probably associated with the outcome.

* So for those of who are epidemiologists, the temptation is to
think, well, we need to adjust for adherence, because it's a confounder.
And I'm going to argue that we should not adjust for adherence.

So we know that people vary with their likelihood to adhere.
Some people receive a treatment from their doctor, and they follow it to a T,
they follow it exactly as it was prescribed.
And other people receive a treatment plan and they don't follow it at all.
The most of us all fall somewhere in between.
So we can think of the likelihood to adhere as a quality that a person has.
And we would expect because of randomization that we'll have
comparable groups at baseline for disquality of likelihood to adhere.

So differencesin adherence after base line are caused by the treatment.
They are part of the treatment affect.
We don't usually adjust for factors that are in the causal pathway.
In the previous figure the arrow was pointing from treatment to adherence.
So adherence is in the causal pathway between treatment and the outcome.
So to adjust for it would essentially be adjusting out
part of the way in which the treatment affects the outcome,
and since randomization has already given us comparable
groups, by adjusting, we may then introduce non-comparability.

* So in this figure, we have a different
situation where adherence is not related to treatment.

So we think treatment is related to the outcome, and
adherence is related to the outcome, but since treatment is
not related to adherence, adherence is not a confounder, so
in this situation, we do not need to adjust for adherence.

So just to recap, if there is an
association between treatment and adherence, then the arrow depicting
this association is going from treatment to adherence, meaning that
adherence is in the causal pathway so we don't adjust.

And if there's no association between treatment
and adherence, then adherence cannot be a confounder
in the relationship between treatment and the outcome so there is no need to adjust.
So adjustment for adherence is not a method that
we used in clinical trials in the primary analyses.

So hopefully I have convinced you
that intention to treat is the appropriate analyses philosophy for
the primary analyses, and that adjustment for adherence is not appropriate.

## Subgroup Analysis

In the context of clinical trials subgroup analyses are analyses where we're looking
to see if there are varying treatment effects in different subsets of patients.
So why do we want to do subgroup analyses?
Well, we'd like to check the consistency of the treatment effect across subgroups.
For instance,
we'd like to know if the effect is the
same in women and men or in children and adults.
Or we might think that perhaps the treatment effect would vary
for people with less severe disease versus those with more severe disease.
And the two most common methods for doing subgroup analyses is
to do stratified analysis or to do a test of interaction.
And finally when we talk about doing a
series of subgroup analysis we need to think
about the problem of multiple comparisons.

### Stratification vs. Test of interaction
 * To do a subgroup analysis by stratification, you estimate
the treatment effect separately in each of the subgroups.
So you estimate an effect in men and you estimate an effect in women.
And using this method, you can test whether there is a significant effect
in men, and you can test if there's a significant effect in women.

But what you cannot do is test whether the
effect in men differs from the effect in women.

So the important point here is that to
test whether there's a treatment effect separately in a
series of subgroups is not the same as testing
whether the treatment effect differs in the various subgroups.

  * The only way to make a statement about whether the treatment effects are
different in one subgroup versus another is to do a formal test for interaction.

So, to do this test we build a statistical model that includes main
effects for our treatment group and for our subgroups and we
use interaction terms to test
for interactions between treatments in subgroup.
And in this way, we can see if the treatment effects vary by subgroup.
We can also use that model to estimate the treatment effects
in the various subgroups like we could do with the stratification method.

### Multiple subgroup analysese
Frequently, when we're do subgroup analyses in a
clinical trial, we aren't doing just one subgroup analysis.
So we aren't only interested in seeing whether
the effects differ in men and women, but
we're also interested to see if the effects
differ across people with different co-morbid conditions at baseline.
Or people who are taking different concomittent medications.
So in practice, we might do a whole series of subgroup analyses.

And when significance tests are performed in multiple subgroups, the
overall Type I error rate for subgroup analyses is inflated.
That means, by chance alone, the probability
that the pay value for a subgroup
difference is less than 0.05 in one or more subgroups is greater than 5%.
So the more subgroup analyses that we do, the more likely it
is by chance alone that we will find a statistically significant difference.

So this is problematic if you use statistical significance as the
only criteria for judging whether or not a difference is true.

To illustrate this point I've included, a graph from a
publication in the New England Journal of Medicine, that shows
the probability of getting a false positive, which is getting
a P value, less than the cutoff just by chance alone.
And on the y-axis
we have the probability of a false positive.
And on the x-axis we have the number of subgroup tests that were performed.

And, you can see that, as you do more
tests the probability of getting a false positive increases.
And, in fact, if you do 40 subgroup tests the probability
of getting at least one false positive is close to 90%.
The probability of getting at least two false positives is about 60% and
the probability of getting at least three false positives is close to 30%.
So the cautionary note here is that we shouldn't get overly excited about
significant findings that pop up when we're doing a series of subgroup analyses.
Instead we need to interpret these, with appropriate skepticism.

### Guidelines for subgroup analysis
So that is, that's not to say that we shouldn't do subgroup analyses.
Clinical trials represent a significant investment of time and
money for the investigators and the sponsors,
and also potential health risks for the participants.
So it's our obligation to gain the maximum amount of
knowledge that we can from the data that we collect.
And it's important to determine if there are subgroups of people
who are more or less likely to be helped by an intervention.

So even if we think that the subgroup
analyses are explorative and not definitive, they can help
to guide future research priorities and they're important.

But how do we perform and report subgroup analysis in a valid and transparent way?
  * Well, as much as possible, the subgroup analysis
should be prespecified and if there are important subgroups,
investigators might consider inflating their sample size, so
that they have a decent power in that subgroup.
  * It's important in publications that investigators report
the number of separate subgroup analyses that
were performed so that the reader has
some idea about the probability of false positives.
  * In some cases, people will adjust for multiple comparisons.
So they'll make an adjustment in the level
of significance that's required to declare a difference significant.

  * And finally, epidemiologists and statisticians
really stress the importance of reporting confidence intervals
instead of or in addition to the p-values,
again so that the reader can see the
uncertainty and the estimate of the subgroup effect.

So these are some guidelines that we think
summarize how to proceed cautiously with sub-group analyses.
